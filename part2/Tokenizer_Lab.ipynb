{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tokenizer Lab: Understanding How Text Becomes Tokens\n",
        "\n",
        "**Goals**\n",
        "- Learn what a tokenizer is and how it maps text to token IDs\n",
        "- Inspect tokens, IDs, and special tokens from the provided tokenizer in `tokenizer/`\n",
        "- Run experiments to see how spaces, punctuation, casing, and numbers affect tokenization\n",
        "- Complete an assignment: Convert your student ID into tokens and analyze the results\n",
        "\n",
        "You will use `transformers.AutoTokenizer` to load the local tokenizer used by the RocLM demo (`roc_demo.py`).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup: load tokenizer from local directory\n",
        "from transformers import AutoTokenizer\n",
        "from pathlib import Path\n",
        "\n",
        "# In notebooks, __file__ is not defined; use current working directory\n",
        "TOKENIZER_PATH = (Path.cwd() / \"tokenizer\").resolve()\n",
        "print(f\"Loading tokenizer from: {TOKENIZER_PATH}\")\n",
        "\n",
        "assert TOKENIZER_PATH.exists(), f\"Tokenizer directory not found at {TOKENIZER_PATH}\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(str(TOKENIZER_PATH), trust_remote_code=True)\n",
        "print(\"Loaded tokenizer!\\n\")\n",
        "\n",
        "# Display basic info\n",
        "print(\"Special tokens:\")\n",
        "print(tokenizer.special_tokens_map)\n",
        "print(\"\\nVocab size:\", len(tokenizer))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1) Basics: Encode and Decode\n",
        "\n",
        "Use the tokenizer to convert text to token IDs and back.\n",
        "- `tokenizer.encode(text)` → list of integers (token IDs)\n",
        "- `tokenizer.decode(ids)` → string\n",
        "- `tokenizer(text, return_tensors=\"pt\")` → ready-to-model tensors\n",
        "\n",
        "Run the following cell and inspect the outputs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample_texts = [\n",
        "    \"Hello world!\",\n",
        "    \"hello world!\",\n",
        "    \"Hello,   world!\",  # extra spaces\n",
        "    \"AI/ML & NLP: 2025\",\n",
        "]\n",
        "\n",
        "for text in sample_texts:\n",
        "    ids = tokenizer.encode(text)\n",
        "    back = tokenizer.decode(ids)\n",
        "    print(\"Text:\", repr(text))\n",
        "    print(\"Token IDs:\", ids)\n",
        "    print(\"Decoded:\", repr(back))\n",
        "    print(\"-\")\n",
        "\n",
        "# Tensor form\n",
        "encoded = tokenizer(\"Hello world!\", return_tensors=\"pt\")\n",
        "print(\"Tensor keys:\", list(encoded.keys()))\n",
        "for k, v in encoded.items():\n",
        "    print(k, v.shape, v)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2) Inspect Tokens and Special Tokens\n",
        "\n",
        "Let's inspect how the tokenizer represents tokens and special tokens.\n",
        "- View the first few and some random token IDs\n",
        "- Inspect special tokens like BOS/EOS, PAD, etc.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from random import sample\n",
        "\n",
        "print(\"Special tokens map:\")\n",
        "for k, v in tokenizer.special_tokens_map.items():\n",
        "    print(f\"  {k}: {repr(v)} -> id {tokenizer.convert_tokens_to_ids(v)}\")\n",
        "\n",
        "print(\"\\nAll special tokens:\")\n",
        "print(tokenizer.all_special_tokens)\n",
        "print(\"All special ids:\")\n",
        "print(tokenizer.all_special_ids)\n",
        "\n",
        "print(\"\\nExamples from vocab:\")\n",
        "example_ids = list(range(10)) + sample(range(100, len(tokenizer)), 10)\n",
        "for tid in example_ids:\n",
        "    tok = tokenizer.convert_ids_to_tokens(tid)\n",
        "    print(f\"  id {tid:6d} -> token {repr(tok)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3) Experiments: What Affects Tokenization?\n",
        "\n",
        "Explore how the tokenizer handles:\n",
        "- Spaces: single vs multiple\n",
        "- Punctuation: commas, slashes, hyphens\n",
        "- Casing: `Hello` vs `hello`\n",
        "- Numbers: `2025`, `000123`, phone-like strings\n",
        "\n",
        "Run the cells and compare token IDs and lengths.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def show_tokenization(text: str):\n",
        "    ids = tokenizer.encode(text)\n",
        "    toks = [tokenizer.convert_ids_to_tokens(i) for i in ids]\n",
        "    print(f\"Text: {repr(text)}\")\n",
        "    print(\"IDs:\", ids)\n",
        "    print(\"TOK:\", toks)\n",
        "    print(\"Len:\", len(ids))\n",
        "    print(\"-\")\n",
        "\n",
        "cases = [\n",
        "    \"Hello world!\",\n",
        "    \"Hello  world!\",  # double space\n",
        "    \"Hello, world!\",\n",
        "    \"Hello-world\",\n",
        "    \"AI/ML\",\n",
        "    \"hello\",\n",
        "    \"Hello\",\n",
        "    \"2025\",\n",
        "    \"000123\",\n",
        "    \"(585) 555-1234\",\n",
        "]\n",
        "\n",
        "for c in cases:\n",
        "    show_tokenization(c)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4) Assignment: Tokenize Your Student ID\n",
        "\n",
        "Follow the instructions below and complete the analysis.\n",
        "\n",
        "1. Enter your student ID as a string (e.g., `\"p1234567\"` or numeric-only if that's your format).\n",
        "2. Tokenize it using the local tokenizer.\n",
        "3. Report:\n",
        "   - The token IDs\n",
        "   - The tokens (string form)\n",
        "   - The number of tokens\n",
        "4. Analyze:\n",
        "   - Does your ID tokenize into a single token or multiple?\n",
        "   - If multiple, why do you think the splits happen where they do?\n",
        "   - Try variants: add a prefix like `\"id:\"`, add spaces, or different casing. What changes?\n",
        "\n",
        "Fill in the next cell and write your observations in the markdown cell after that.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Student Work ===\n",
        "# Replace with your own student ID string\n",
        "student_id = \"p1234567\"  # <-- change me\n",
        "\n",
        "ids = tokenizer.encode(student_id)\n",
        "toks = [tokenizer.convert_ids_to_tokens(i) for i in ids]\n",
        "\n",
        "print(\"Student ID:\", student_id)\n",
        "print(\"Token IDs:\", ids)\n",
        "print(\"Tokens:\", toks)\n",
        "print(\"Num tokens:\", len(ids))\n",
        "\n",
        "# Try some variants\n",
        "variants = [\n",
        "    student_id,\n",
        "    f\"id:{student_id}\",\n",
        "    f\"ID:{student_id}\",\n",
        "    f\"{student_id} \",  # trailing space\n",
        "    f\" {student_id}\",  # leading space\n",
        "]\n",
        "\n",
        "print(\"\\nVariants:\")\n",
        "for v in variants:\n",
        "    v_ids = tokenizer.encode(v)\n",
        "    v_toks = [tokenizer.convert_ids_to_tokens(i) for i in v_ids]\n",
        "    print(f\"{repr(v)} -> {v_ids} -> {v_toks}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Analysis (write your observations here)\n",
        "\n",
        "- Is your ID a single token or multiple? Why?\n",
        "- How do prefixes (`id:` vs `ID:`) change the result?\n",
        "- What effect do leading/trailing spaces have?\n",
        "- Relate this to subword tokenization (e.g., BPE/WordPiece) and vocabulary coverage.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
